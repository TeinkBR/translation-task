{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeinkBR/translation-task/blob/main/Intento_python_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0MZMX-Vhm4T"
      },
      "source": [
        "Reference:\n",
        "\n",
        "\n",
        "https://realpython.com/python-nltk-sentiment-analysis/\n",
        "\n",
        "https://dziopa.com/blog/1-getting-started-nlp/\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/07/nltk-a-beginners-hands-on-guide-to-natural-language-processing/\n",
        "\n",
        "https://www.analyticssteps.com/blogs/nltk-python-tutorial-beginners\n",
        "\n",
        "https://www.guru99.com/nltk-tutorial.html\n",
        "\n",
        "https://www.nltk.org/api/nltk.tokenize.html\n",
        "\n",
        "https://www.nltk.org/book/ch01.html\n",
        "\n",
        "http://morfeusz.sgjp.pl/\n",
        "\n",
        "https://www.datacamp.com/tutorial/text-analytics-beginners-nltk\n",
        "\n",
        "https://www.geeksforgeeks.org/snowball-stemmer-nlp/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ8sjl_VoLyS"
      },
      "outputs": [],
      "source": [
        "!pip install tmx2dataframe --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6dM12ceJ4kw"
      },
      "outputs": [],
      "source": [
        "!pip install bs4 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BllV3Li3J5Ko"
      },
      "outputs": [],
      "source": [
        "!pip install requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBm5PL2Dh1DT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4euDOEyVFIX"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEVaNoHH5Qgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d51a39e-abed-40d3-ae03-7180bb2fb3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELPwA0CLeBKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac0656b-c7c4-4111-dfa6-f205f4608faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#import tmx2dataframe\n",
        "from tmx2dataframe import tmx2dataframe\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "data_dir = '/content/gdrive/My Drive/Colab Notebooks/Analytical tasks from Intento/data-1/en-pl.tmx'\n",
        "\n",
        "metadata, df = tmx2dataframe.read(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABgHHKXzhLCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fa06d6-96b9-4ea5-d052-e0c35c4dd195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of metadata ()\n",
            "type of metadata <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print('shape of metadata',np.shape(metadata))\n",
        "metadata= np.array(metadata)\n",
        "print('type of metadata',type(metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTmTdMaRKPGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf24082-33f1-4e25-bf73-9bf821cb951e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meta data ()\n"
          ]
        }
      ],
      "source": [
        "print('meta data',np.shape(metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PfqIw8-KfgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08db361f-b944-4f7f-b993-e8b4f3861b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meta data 0    {'creationdate': 'Sun Mar  4 00:57:39 2018', '...\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print('meta data',pd.Series(metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tBNH6qXhvrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456a306b-2612-49f8-8ea0-77f3062abe76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df        source_language                                    source_sentence  \\\n",
            "0                   en  http://www.ted.com/talks/stephen_palumbi_follo...   \n",
            "1                   en  There's a tight and surprising link between th...   \n",
            "2                   en            fish,health,mission blue,oceans,science   \n",
            "3                   en                                                899   \n",
            "4                   en       Stephen Palumbi: Following the mercury trail   \n",
            "...                ...                                                ...   \n",
            "148043              en  It's a perfect image for the fundamental point...   \n",
            "148044              en  No one person, no one alliance, no one nation,...   \n",
            "148045              en  The vision statement of Wikipedia is very simp...   \n",
            "148046              en  My thesis for you is that by combining interna...   \n",
            "148047              en         Thank you very much. Thank you. Thank you.   \n",
            "\n",
            "       target_language                                    target_sentence  \n",
            "0                   pl  http://www.ted.com/talks/lang/pl/stephen_palum...  \n",
            "1                   pl  Według biologa Stephena Palumbiego istnieje si...  \n",
            "2                   pl            fish,health,mission blue,oceans,science  \n",
            "3                   pl                                                899  \n",
            "4                   pl      Stephen Palumbi: Podążając rtęciowym szlakiem  \n",
            "...                ...                                                ...  \n",
            "148043              pl  To doskonały przykład sedna sprawy, że żaden z...  \n",
            "148044              pl  Żadna osoba, żaden sojusz, żaden kraj, nikt ni...  \n",
            "148045              pl  Motto Wikipedii jest bardzo proste: świat, w k...  \n",
            "148046              pl  Moją tezą jest, że łącząc strategiczną komunik...  \n",
            "148047              pl                         Bardzo dziękuję. Dziękuję.  \n",
            "\n",
            "[148048 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "print('df',pd.DataFrame(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjEebDF4B1wM"
      },
      "source": [
        "We can see from the text that it is already been seperated, some hyperlinks are better to be removed. \n",
        "\n",
        "To create a glossary we better first tokenize it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoOXoa75CguJ"
      },
      "source": [
        "to tokenize we use nltk library again, first seperate the polish part and english part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlYc9xWCoU2n"
      },
      "outputs": [],
      "source": [
        "polnisch=df['target_sentence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgHQeFruoHJj"
      },
      "outputs": [],
      "source": [
        "englisch=df['source_sentence']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "polnisch_after_tokenize=polnisch.apply(word_tokenize)"
      ],
      "metadata": {
        "id": "Slb_xA_0J_0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "englisch_after_tokenize=englisch.apply(word_tokenize)"
      ],
      "metadata": {
        "id": "32lf4rKgKL1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(englisch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "bVJkSHTcmD_A",
        "outputId": "844ec58a-9e1f-4118-ac18-00e39b132971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          source_sentence\n",
              "0       http://www.ted.com/talks/stephen_palumbi_follo...\n",
              "1       There's a tight and surprising link between th...\n",
              "2                 fish,health,mission blue,oceans,science\n",
              "3                                                     899\n",
              "4            Stephen Palumbi: Following the mercury trail\n",
              "...                                                   ...\n",
              "148043  It's a perfect image for the fundamental point...\n",
              "148044  No one person, no one alliance, no one nation,...\n",
              "148045  The vision statement of Wikipedia is very simp...\n",
              "148046  My thesis for you is that by combining interna...\n",
              "148047         Thank you very much. Thank you. Thank you.\n",
              "\n",
              "[148048 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54417f94-734a-49be-90eb-c8f6d293e5ee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.ted.com/talks/stephen_palumbi_follo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>There's a tight and surprising link between th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fish,health,mission blue,oceans,science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Stephen Palumbi: Following the mercury trail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148043</th>\n",
              "      <td>It's a perfect image for the fundamental point...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148044</th>\n",
              "      <td>No one person, no one alliance, no one nation,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148045</th>\n",
              "      <td>The vision statement of Wikipedia is very simp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148046</th>\n",
              "      <td>My thesis for you is that by combining interna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148047</th>\n",
              "      <td>Thank you very much. Thank you. Thank you.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148048 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54417f94-734a-49be-90eb-c8f6d293e5ee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-54417f94-734a-49be-90eb-c8f6d293e5ee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-54417f94-734a-49be-90eb-c8f6d293e5ee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,i_token in enumerate(polnisch_after_tokenize):\n",
        "  polnisch_after_tokenize=polnisch_after_tokenize[i]-polnisch_after_tokenize[0]\n",
        "\n",
        "\n",
        "polnisch_remove_link=pd.DataFrame(polnisch_after_tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "VEOz8dOPL4Fl",
        "outputId": "0387cd3b-4baa-408e-c442-df6df2ea79db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-e49c62595af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolnisch_after_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mpolnisch_after_tokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolnisch_after_tokenize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpolnisch_after_tokenize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpolnisch_remove_link\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolnisch_after_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2EZJMMBgkgd"
      },
      "outputs": [],
      "source": [
        "polnisch_string=polnisch.to_string()\n",
        "englisch_string=englisch.to_string()\n",
        "import re\n",
        "polnisch_string = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', polnisch_string, flags=re.MULTILINE)\n",
        "englisch_string = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', englisch_string, flags=re.MULTILINE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "polnisch_string"
      ],
      "metadata": {
        "id": "YwRnGx2eLc4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polnisch_series=pd.Series(polnisch_string)"
      ],
      "metadata": {
        "id": "y2nE9uBsLmKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polnisch_series"
      ],
      "metadata": {
        "id": "-wEj-7G3LrwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.frame=pd.DataFrame(polnisch_string)"
      ],
      "metadata": {
        "id": "UVUDSpw-Ltb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydeepl --quiet "
      ],
      "metadata": {
        "id": "VGDEfh2pXpJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pydeepl\n",
        "\n",
        "sentence = polnisch[0]\n",
        "from_language = 'PL'\n",
        "to_language = 'EN'\n",
        "\n",
        "translation = pydeepl.translate(sentence, to_language, from_lang=from_language)\n",
        "print(translation)\n",
        "\n",
        "# Using auto-detection\n",
        "translation = pydeepl.translate(sentence, to_language)\n",
        "print(translation)\n"
      ],
      "metadata": {
        "id": "MiSP-b2tXVLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "u0NmhhlbYOU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polnisch_series = pd.Series(polnisch_string)\n",
        "englisch_series = pd.Series(englisch_string)"
      ],
      "metadata": {
        "id": "tq6hDgYENmga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import inflect\n",
        "input_text = polnisch_string\n",
        "output_text =  BeautifulSoup(input_text, \"html.parser\").get_text()\n",
        "print('Input: ' + input_text)\n",
        "print('Output: ' + output_text)"
      ],
      "metadata": {
        "id": "R_RwlL82dAcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "lVlwtQk8dHDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('df',pd.DataFrame(polnisch_series))"
      ],
      "metadata": {
        "id": "rhcUqvGANuJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polnisch_series=polnisch_series[2:-1]"
      ],
      "metadata": {
        "id": "Yg4MzlxCQcCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hyper link removed"
      ],
      "metadata": {
        "id": "8hbaZMQOSjWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('df',pd.DataFrame(polnisch_series))"
      ],
      "metadata": {
        "id": "dF23SDoJSdVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polnisch_series_to_string=polnisch_series.to_string()"
      ],
      "metadata": {
        "id": "-RYPJt5YSp7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_en = nltk.SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "K_PqRQpOUhe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polnisch_after_tokenize[0] --remove"
      ],
      "metadata": {
        "id": "uiVKDBEv28zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_en(englisch)"
      ],
      "metadata": {
        "id": "EWcbiahiyXya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "stemmed_word =  stemmer.stem(englisch_series)\n",
        "lemmatised_word = lemmatizer.lemmatize(englisch_series)"
      ],
      "metadata": {
        "id": "sYV0d03PT84f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import these modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "  \n",
        "lemmatizer.polnisch_series_to_string"
      ],
      "metadata": {
        "id": "7dpI9AGqMhc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dYIJcnd5BXY"
      },
      "source": [
        "test for an example sentence in the paragraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttgZ6LRuHToV"
      },
      "source": [
        "Now we need tokenization to make each invididual word in a sentence a seperate token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8fzHrf9H3Xw"
      },
      "source": [
        "Now we try to output single words by token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2gukHCPIAl_"
      },
      "source": [
        "It worked, but more sophiscated method is needed to set up a glossary for comparision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13PiYbK9rKv9"
      },
      "source": [
        "Snowball stemmer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKl0oCdVrIG-"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mowlThMQpZJ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "aPASbahg6-Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(polnisch_string):\n",
        "   fdist[word.lower()] += 1"
      ],
      "metadata": {
        "id": "j_OQNdZb64fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yii_9SiEm1X5"
      },
      "outputs": [],
      "source": [
        "print('type of fdistribution',type(fdistribution))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU6qsBalyxB6"
      },
      "outputs": [],
      "source": [
        "!pip install snowballstemmer --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WHDNfGvzBiS"
      },
      "outputs": [],
      "source": [
        "!pip install morfeusz2 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgLShiJR3h2F"
      },
      "outputs": [],
      "source": [
        "!pip install pystempel --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45F5LYxv3mYW"
      },
      "outputs": [],
      "source": [
        "from stempel import StempelStemmer\n",
        "stemmer = StempelStemmer.default()\n",
        "stemmer = StempelStemmer.polimorf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMvXwd7f3s7v"
      },
      "outputs": [],
      "source": [
        "stemmed_polnisch=[stemmer.stem(word) for i_word, word in enumerate(polnisch_series)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_polnisch[0]"
      ],
      "metadata": {
        "id": "jUe8c06X4b6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx4k4OoaphXm"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5id_1OxHmR_1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "ps = PorterStemmer()\n",
        " \n",
        "#sentence = \"Programmers program with programming languages\"\n",
        "words = word_tokenize(polnisch)\n",
        " \n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQXNI3dFpyIJ"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nMdNc3vsk7D"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "# POS (Parts Of Speech) for: nouns, adjectives, verbs and adverbs\n",
        "DI_POS_TYPES = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'} \n",
        "POS_TYPES = list(DI_POS_TYPES.keys())\n",
        "\n",
        "# Constraints on tokens\n",
        "MIN_STR_LEN = 3\n",
        "RE_VALID = '[a-zA-Z]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4uPTj19saxc"
      },
      "outputs": [],
      "source": [
        "# Get stopwords, stemmer and lemmatizer\n",
        "stopwords = nltk.corpus.stopwords.words('polish')\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# Remove accents function\n",
        "def remove_accents(data):\n",
        "    return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "# Process all quotes\n",
        "li_tokens = []\n",
        "li_token_lists = []\n",
        "li_lem_strings = []\n",
        "\n",
        "for i,text in enumerate(englisch):\n",
        "    # Tokenize by sentence, then by lowercase word\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "\n",
        "    # Process all tokens per quote\n",
        "    li_tokens_quote = []\n",
        "    li_tokens_quote_lem = []\n",
        "    for token in tokens:\n",
        "        # Remove accents\n",
        "        t = remove_accents(token)\n",
        "\n",
        "        # Remove punctuation\n",
        "        t = str(t).translate(string.punctuation)\n",
        "        li_tokens_quote.append(t)\n",
        "        \n",
        "        # Add token that represents \"no lemmatization match\"\n",
        "        li_tokens_quote_lem.append(\"-\") # this token will be removed if a lemmatization match is found below\n",
        "\n",
        "        # Process each token\n",
        "        if t not in stopwords:\n",
        "            if re.search(RE_VALID, t):\n",
        "                if len(t) >= MIN_STR_LEN:\n",
        "                    # Note that the POS (Part Of Speech) is necessary as input to the lemmatizer \n",
        "                    # (otherwise it assumes the word is a noun)\n",
        "                    pos = nltk.pos_tag([t])[0][1][:2]\n",
        "                    pos2 = 'n'  # set default to noun\n",
        "                    if pos in DI_POS_TYPES:\n",
        "                      pos2 = DI_POS_TYPES[pos]\n",
        "                    \n",
        "                    stem = stemmer.stem(t)\n",
        "                    lem = lemmatizer.lemmatize(t, pos=pos2)  # lemmatize with the correct POS\n",
        "                    \n",
        "                    if pos in POS_TYPES:\n",
        "                        li_tokens.append((t, stem, lem, pos))\n",
        "\n",
        "                        # Remove the \"-\" token and append the lemmatization match\n",
        "                        li_tokens_quote_lem = li_tokens_quote_lem[:-1] \n",
        "                        li_tokens_quote_lem.append(lem)\n",
        "\n",
        "    # Build list of token lists from lemmatized tokens\n",
        "    li_token_lists.append(li_tokens_quote)\n",
        "    \n",
        "    # Build list of strings from lemmatized tokens\n",
        "    str_li_tokens_quote_lem = ' '.join(li_tokens_quote_lem)\n",
        "    li_lem_strings.append(str_li_tokens_quote_lem)\n",
        "    \n",
        "# Build resulting dataframes from lists\n",
        "df_token_lists = pd.DataFrame(li_token_lists)\n",
        "\n",
        "print(\"df_token_lists.head(5):\")\n",
        "print(df_token_lists.head(5).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXrLO0lupTRr"
      },
      "outputs": [],
      "source": [
        "word_tokenize(englisch[:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAxbNsccigS-"
      },
      "outputs": [],
      "source": [
        "pip install googletrans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5JTa4ATirHa"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "translator.translate(eng[:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khcWrcR4mHIL"
      },
      "outputs": [],
      "source": [
        "a = np.array([[1, 2], [3, 4]])\n",
        "b = np.array([[5, 6]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.concatenate((a, b), axis=0)"
      ],
      "metadata": {
        "id": "uiWzmaLtBO0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.concatenate((a, b.T), axis=1)"
      ],
      "metadata": {
        "id": "OuotkCc2BFiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk import LancasterStemmer"
      ],
      "metadata": {
        "id": "wmn96mhgBRZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['sincerely','electricity','roughly','ringing']\n",
        "\n",
        "Lanc = LancasterStemmer()\n",
        "\n",
        "for w in words:\n",
        "\n",
        "    print(w, \" : \", Lanc.stem(w))"
      ],
      "metadata": {
        "id": "5TJ8KoVbeF6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in polnisch_after_tokenize:\n",
        "\n",
        "    print(w, \" : \", Lanc.stem(w))"
      ],
      "metadata": {
        "id": "4Eo5mv02eH04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.10.0. --quiet"
      ],
      "metadata": {
        "id": "BTtkpt3Tp4oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "5tpyCl3jeN4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "4XxIyO6qp-oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IIIFwJjfpauV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNLWCAnqbgDiPY5ncQYLKhf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}